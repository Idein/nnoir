#!/usr/bin/env python3
import io
import sys
import copy
import functools
import numpy as np
import argparse
import onnx
import onnxruntime
import msgpack
from itertools import chain
from mlir import *
from mlir.edges import *

class Function:

    def __init__(self, name, params = {}):
        self.name = name
        self.params = params

    def to_dict(self):
        return {
            b'name': self.name,
            b'params': self.params
        }

    def __str__(self):
        return '{}'.format(self.name)

def tensor_to_narray(tensor):
    arr = []
    storage = onnx.mapping.TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE[tensor.data_type]
    storage = onnx.mapping.STORAGE_TENSOR_TYPE_TO_FIELD[storage]
    arr = getattr(tensor, storage)
    if arr == []:
        result = np.frombuffer(tensor.raw_data, dtype=onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[tensor.data_type])
    else:
        result = np.array(arr, dtype=onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[tensor.data_type])
    shape = tensor.dims if tensor.dims != [] else [1]
    return result.reshape(*shape)

def narray_to_value_info(name, arr):
    return onnx.helper.make_tensor_value_info(name, onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[arr.dtype], arr.shape)

def encode_ndarray(obj):
    if obj is None:
        return None
    else:
        with io.BytesIO() as out:
            np.save(out, obj.copy())
            return { b'ndarray': out.getvalue() }

def auto_pad_to_manual_pad(n, k, s, d, auto_pad):
    dk = (k - 1) * d + 1
    if n % s == 0:
        pad = max(dk - s, 0)
    else:
        pad = max(dk - n % s, 0)
    if auto_pad == b'SAME_LOWER':
        pad_before = pad // 2
        pad_after  = pad - pad_before
        return (pad_before, pad_after)
    elif auto_pad == b'SAME_UPPER':
        pad_after = pad // 2
        pad_before  = pad - pad_after
        return (pad_before, pad_after)
    elif auto_pad == b'VALID':
        return (0, 0)
    else:
        raise 'invalid'

class UnsupportedONNXOperation(Exception):
    def __init__(self, node, message):
        self.node = node
        self.message = message

class Op:
    def __init__(self, node):
        self.node = node

    def get_dummy_output(self, env):
        raise UnsupportedONNXOperation(self.node, "not implemented")

    def to_Edge(self, env, constants):
        raise UnsupportedONNXOperation(self.node, "not implemented")

class OpAdd(Op):

    def __init__(self, node):
        super().__init__(node)

    def get_dummy_output(self, env):
        [a,b] = self.node.input
        return env[a] + env[b]

    def to_Edge(self, env, constants):
        [a, b] = self.node.input
        if a in constants and b not in constants:
            return [ Bias([b], list(self.node.output), axis=0,b=encode_ndarray(constants[a])) ]
        elif a not in constants and b in constants:
            return [ Bias([a], list(self.node.output), axis=0,b=encode_ndarray(constants[b])) ]
        elif a not in constants and b not in constants:
            return [ Add(list(self.node.input), list(self.node.output)) ]
        else:
            raise UnsupportedONNXOperation(self.node, 'bug! (unreachable here)')

class OpAveragePool(Op):

    def __init__(self, node):
        super().__init__(node)

        self.kernel_shape = None
        self.auto_pad = b'NOTSET'
        self.pads = None
        self.storage_order = 0
        self.strides = (1,1)
        self.count_include_pad = 0
        for attr in node.attribute:
            if attr.name == 'kernel_shape':
                self.kernel_shape = attr.ints
            if attr.name == 'storage_order':
                self.storage_order = attr.i
            if attr.name == 'strides':
                self.strides = attr.ints
            if attr.name == 'auto_pad':
                self.auto_pad = attr.s
            if attr.name == 'pads':
                self.pads = attr.ints
            if attr.name == 'count_include_pad':
                self.count_include_pad = attr.i

    def get_dummy_output(self, env):
        [x] = self.node.input

        _input = env[x]
        batch = _input.shape[0]
        channel = _input.shape[1]
        in_h = _input.shape[2]
        in_w = _input.shape[3]
        kh = self.kernel_shape[0]
        kw = self.kernel_shape[1]
        sy = self.strides[0]
        sx = self.strides[1]

        if self.auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if self.pads is not None:
                pad_h = (self.pads[0], self.pads[2])
                pad_w = (self.pads[1], self.pads[3])
        else:
            pad_h = auto_pad_to_manual_pad(in_h, kh, sy, 1, self.auto_pad)
            pad_w = auto_pad_to_manual_pad(in_w, kw, sx, 1, self.auto_pad)

        out_h = ((pad_h[0] + in_h + pad_h[1]) - ((kh - 1) * 1 + 1)) // sy + 1
        out_w = ((pad_w[0] + in_w + pad_w[1]) - ((kw - 1) * 1 + 1)) // sx + 1

        return np.zeros((batch, channel, out_h, out_w), dtype=env[x].dtype)

    def to_Edge(self, env, constants):
        [x] = self.node.input

        _input = env[x]
        batch = _input.shape[0]
        channel = _input.shape[1]
        in_h = _input.shape[2]
        in_w = _input.shape[3]
        kh = self.kernel_shape[0]
        kw = self.kernel_shape[1]
        sy = self.strides[0]
        sx = self.strides[1]

        if self.auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if self.pads is not None:
                pad_h = (self.pads[0], self.pads[2])
                pad_w = (self.pads[1], self.pads[3])
        else:
            pad_h = auto_pad_to_manual_pad(in_h, kh, sy, 1, self.auto_pad)
            pad_w = auto_pad_to_manual_pad(in_w, kw, sx, 1, self.auto_pad)

        return [
            AveragePooling2D(
                list(self.node.input),
                list(self.node.output),
                kernel=(kh, kw),
                stride=(sy, sx),
                pad_h=pad_h,
                pad_w=pad_w,
                count_exclude_pad=self.count_include_pad==0,
            )
        ]

class OpBatchNormalization(Op):
    def __init__(self, node):
        super().__init__(node)

    def get_dummy_output(self, env):
        [x,_,_,_,_] = self.node.input
        return env[x]

    def to_Edge(self, env, constants):
        [x,gamma,beta,mean,var] = self.node.input
        if gamma not in constants:
            raise UnsupportedONNXOperation(self.node, 'missing gamma')
        if beta not in constants:
            raise UnsupportedONNXOperation(self.node, 'missing beta')
        if mean not in constants:
            raise UnsupportedONNXOperation(self.node, 'missing mean')
        if var not in constants:
            raise UnsupportedONNXOperation(self.node, 'missing var')
        eps = 1e-05
        for attr in self.node.attribute:
            if attr.name == 'epsilon':
                eps = attr.f
        return [
            BatchNormalization(
                [x],
                list(self.node.output),
                eps=eps,
                avg_mean=encode_ndarray(constants[mean]),
                avg_var=encode_ndarray(constants[var]),
                gamma=encode_ndarray(constants[gamma]),
                beta=encode_ndarray(constants[beta]),
            )
        ]

class OpClip(Op):

    def __init__(self, node):
        super().__init__(node)

    def get_dummy_output(self, env):
        [x] = self.node.input
        return env[x]

    def to_Edge(self, env, constants):
        _min = -3.4028234663852886e+38
        _max =  3.4028234663852886e+38
        for attr in self.node.attribute:
            if attr.name == 'max':
                _max = attr.f
            if attr.name == 'min':
                _min = attr.f

        if _min != 0.0:
            raise UnsupportedONNXOperation(self.node, 'min must be 0.0')

        return [
            ClippedReLU(
                list(self.node.input),
                list(self.node.output),
                upper=_max
            )
        ]

class OpConcat(Op):

    def __init__(self, node):
        super().__init__(node)

        self.axis = None
        for attr in node.attribute:
            if attr.name == 'axis':
                self.axis = attr.i

    def get_dummy_output(self, env):
        return np.concatenate(list(map(lambda x: env[x], self.node.input)), axis=self.axis)

    def to_Edge(self, env, constants):
        return [
            Concat(
                list(self.node.input),
                list(self.node.output),
                axis=self.axis
            )
        ]

class OpConv(Op):

    def __init__(self, node):
        super().__init__(node)

        self.kernel_shape = None
        self.auto_pad = b'NOTSET'
        self.pads = None
        self.strides = (1,1)
        self.dilations = (1,1)
        self.group = 1
        for attr in node.attribute:
            if attr.name == 'kernel_shape':
                self.kernel_shape = attr.ints
            if attr.name == 'dilations':
                self.dilations = attr.ints
            if attr.name == 'group':
                self.group = attr.i
            if attr.name == 'strides':
                self.strides = attr.ints
            if attr.name == 'auto_pad':
                self.auto_pad = attr.s
            if attr.name == 'pads':
                self.pads = attr.ints

    def get_dummy_output(self, env):

        if len(self.node.input) == 3:
            [x,W,b] = self.node.input
        else:
            [x,W] = self.node.input
            b = None

        _input = env[x]
        batch = _input.shape[0]
        in_ch = _input.shape[1]
        in_h = _input.shape[2]
        in_w = _input.shape[3]
        kh = self.kernel_shape[0]
        kw = self.kernel_shape[1]
        sy = self.strides[0]
        sx = self.strides[1]
        dy = self.dilations[0]
        dx = self.dilations[1]

        if self.auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if self.pads is not None:
                pad_h = (self.pads[0], self.pads[2])
                pad_w = (self.pads[1], self.pads[3])
        else:
            pad_h = auto_pad_to_manual_pad(in_h, kh, sy, dy, self.auto_pad)
            pad_w = auto_pad_to_manual_pad(in_w, kw, sx, dx, self.auto_pad)

        out_ch = env[W].shape[0]

        out_h = ((pad_h[0] + in_h + pad_h[1]) - ((kh - 1) * dy + 1)) // sy + 1
        out_w = ((pad_w[0] + in_w + pad_w[1]) - ((kw - 1) * dx + 1)) // sx + 1

        return np.zeros((batch, out_ch, out_h, out_w), dtype=env[x].dtype)

    def to_Edge(self, env, constants):
        b = None
        if len(self.node.input) == 2:
            [x, W] = self.node.input
        elif len(self.node.input) == 3:
            [x, W, b] = self.node.input
        else:
            raise 'invalid'
        if W not in constants:
            raise UnsupportedONNXOperation(self.node, 'W must be constant')
        W = constants[W]
        if b is not None:
            if b not in constants:
                raise UnsupportedONNXOperation(self.node, 'b must be constant')
            b = constants[b]

        _input = env[x]
        batch = _input.shape[0]
        in_ch = _input.shape[1]
        in_h = _input.shape[2]
        in_w = _input.shape[3]
        kh = self.kernel_shape[0]
        kw = self.kernel_shape[1]
        sy = self.strides[0]
        sx = self.strides[1]
        dy = self.dilations[0]
        dx = self.dilations[1]

        if self.auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if self.pads is not None:
                pad_h = (self.pads[0], self.pads[2])
                pad_w = (self.pads[1], self.pads[3])
        else:
            pad_h = auto_pad_to_manual_pad(in_h, kh, sy, dy, self.auto_pad)
            pad_w = auto_pad_to_manual_pad(in_w, kw, sx, dx, self.auto_pad)

        is_depthwise = False
        out_channels,in_channels_per_groups = W.shape[:2]
        if self.group > 1 and 1 == in_channels_per_groups:
            return [
                DepthwiseConvolution2D(
                    [x],
                    list(self.node.output),
                    W=encode_ndarray(np.rollaxis(W.reshape(self.group,out_channels//self.group,kh,kw),1,0)),
                    b=encode_ndarray(b),
                    stride=(sy, sx),
                    pad_h=pad_h,
                    pad_w=pad_w,
                    dilate=(dy, dx),
                )
            ]
        else:
            return [
                Convolution2D(
                    [x],
                    list(self.node.output),
                    W=encode_ndarray(W),
                    b=encode_ndarray(b),
                    stride=(sy, sx),
                    pad_h=pad_h,
                    pad_w=pad_w,
                    dilate=(dy, dx),
                    groups=self.group
                )
            ]

class OpDropout(Op):

    def __init__(self, node):
        super().__init__(node)

    def get_dummy_output(self, env):
        [x] = self.node.input
        return env[x]

    def to_Edge(self, env, constants):
        return [
            Transpose(
                list(self.node.input),
                list(self.node.output[:1]),
                axes=list(range(len(env[self.node.input[0]].shape)))
            )
        ]

class OpGemm(Op):

    def __init__(self, node):
        super().__init__(node)

        self.alpha = 1.0
        self.beta = 1.0
        self.transA = 0
        self.transB = 0
        for attr in node.attribute:
            if attr.name == 'alpha':
                self.alpha = attr.f
            if attr.name == 'beta':
                self.beta = attr.f
            if attr.name == 'transA':
                self.transA = attr.i
            if attr.name == 'transB':
                self.transB = attr.i

    def get_dummy_output(self, env):
        [A,B,C] = self.node.input
        a = env[A]
        if self.transA == 1:
            a = a.T
        b = env[B]
        if self.transB == 1:
            b = b.T
        return a.dot(b)+env[C]

    def to_Edge(self, env, constants):
        [A,B,C] = self.node.input

        if B not in constants:
            raise UnsupportedONNXOperation(self.node, 'B must be constant')
        if C not in constants:
            raise UnsupportedONNXOperation(self.node, 'C must be constant')

        b = env[B]
        if self.transB == 0:
            b = b.T
        c = env[C]

        if len(c.shape) == 2 and c.shape[0] != 1:
            raise UnsupportedONNXOperation(self.node, 'shapes mismatch')

        if self.transA == 1:
            internal_node = "{}_{}".format(A, id(A))
            env[internal_node] = env[A].T
            return [
                Transpose(
                    [A],
                    [internal_node],
                    axes=[1,0]
                ),
                Linear(
                    [internal_node],
                    list(self.node.output),
                    W=encode_ndarray(self.alpha * b),
                    b=encode_ndarray(self.beta * c.ravel())
                )
            ]
        else:
            return [
                Linear(
                    [A],
                    list(self.node.output),
                    W=encode_ndarray(self.alpha * b),
                    b=encode_ndarray(self.beta * c.ravel())
                )
            ]

class OpLRN(Op):

    def __init__(self, node):
        super().__init__(node)

        self.alpha = 0.0001
        self.beta = 0.75
        self.bias = 1.0
        self.size = None
        for attr in node.attribute:
            if attr.name == 'alpha':
                self.alpha = attr.f
            if attr.name == 'beta':
                self.beta = attr.f
            if attr.name == 'bias':
                self.bias = attr.f
            if attr.name == 'size':
                self.size = attr.i

    def get_dummy_output(self, env):
        [x] = self.node.input
        return env[x]

    def to_Edge(self, env, constants):
        return [
            LocalResponseNormalization(
                list(self.node.input),
                list(self.node.output),
                n=self.size,
                k=self.bias,
                alpha=self.alpha,
                beta=self.beta
            )
        ]

class OpMatMul(Op):

    def __init__(self, node):
        super().__init__(node)

    def get_dummy_output(self, env):
        [x,W] = self.node.input
        return env[x].dot(env[W])

    def to_Edge(self, env, constants):
        [x,W] = self.node.input
        if W in constants:
            return [
                Linear(
                    [x],
                    list(self.node.output),
                    W=encode_ndarray(env[W]),
                    b=None
                )
            ]
        else:
            raise UnsupportedONNXOperation(self.node, 'W must be constant')

class OpMaxPool(Op):

    def __init__(self, node):
        super().__init__(node)

        self.kernel_shape = None
        self.auto_pad = b'NOTSET'
        self.pads = None
        self.storage_order = 0
        self.strides = (1,1)
        for attr in node.attribute:
            if attr.name == 'kernel_shape':
                self.kernel_shape = attr.ints
            if attr.name == 'storage_order':
                self.storage_order = attr.i
            if attr.name == 'strides':
                self.strides = attr.ints
            if attr.name == 'auto_pad':
                self.auto_pad = attr.s
            if attr.name == 'pads':
                self.pads = attr.ints

    def get_dummy_output(self, env):
        [x] = self.node.input

        _input = env[x]
        batch = _input.shape[0]
        channel = _input.shape[1]
        in_h = _input.shape[2]
        in_w = _input.shape[3]
        kh = self.kernel_shape[0]
        kw = self.kernel_shape[1]
        sy = self.strides[0]
        sx = self.strides[1]

        if self.auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if self.pads is not None:
                pad_h = (self.pads[0], self.pads[2])
                pad_w = (self.pads[1], self.pads[3])
        else:
            pad_h = auto_pad_to_manual_pad(in_h, kh, sy, 1, self.auto_pad)
            pad_w = auto_pad_to_manual_pad(in_w, kw, sx, 1, self.auto_pad)

        out_h = ((pad_h[0] + in_h + pad_h[1]) - ((kh - 1) * 1 + 1)) // sy + 1
        out_w = ((pad_w[0] + in_w + pad_w[1]) - ((kw - 1) * 1 + 1)) // sx + 1

        return np.zeros((batch, channel, out_h, out_w), dtype=env[x].dtype)

    def to_Edge(self, env, constants):
        [x] = self.node.input

        _input = env[x]
        batch = _input.shape[0]
        channel = _input.shape[1]
        in_h = _input.shape[2]
        in_w = _input.shape[3]
        kh = self.kernel_shape[0]
        kw = self.kernel_shape[1]
        sy = self.strides[0]
        sx = self.strides[1]

        if self.auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if self.pads is not None:
                pad_h = (self.pads[0], self.pads[2])
                pad_w = (self.pads[1], self.pads[3])
        else:
            pad_h = auto_pad_to_manual_pad(in_h, kh, sy, 1, self.auto_pad)
            pad_w = auto_pad_to_manual_pad(in_w, kw, sx, 1, self.auto_pad)

        return [
            MaxPooling2D(
                list(self.node.input),
                list(self.node.output),
                kernel=(kh, kw),
                stride=(sy, sx),
                pad_h=pad_h,
                pad_w=pad_w,
            )
        ]

class OpPad(Op):

    def __init__(self, node):
        super().__init__(node)

        self.mode = b'constant'
        self.pads = None
        self.value = 0.0
        for attr in node.attribute:
            if attr.name == 'mode':
                self.mode = attr.s
            if attr.name == 'pads':
                self.pads = attr.ints
            if attr.name == 'value':
                self.value = attr.f

        if self.mode != b'constant':
            raise UnsupportedONNXOperation(self.node, 'mode must be "constant"')

    def get_dummy_output(self, env):
        [x] = self.node.input
        n = len(self.pads) // 2
        return np.pad(env[x], list(zip(self.pads[:n], self.pads[n:])), mode='constant', constant_values=self.value)

    def to_Edge(self, env, constants):
        n = len(self.pads) // 2
        return [
            ConstantPadding(
                list(self.node.input),
                list(self.node.output),
                pads=list(zip(self.pads[:n], self.pads[n:])),
                value=self.value,
            )
        ]

class OpRelu(Op):

    def __init__(self, node):
        super().__init__(node)

    def get_dummy_output(self, env):
        [x] = self.node.input
        return env[x]

    def to_Edge(self, env, constants):
        return [ ReLU(list(self.node.input), list(self.node.output)) ]

class OpReshape(Op):

    def __init__(self, node):
        super().__init__(node)

    def get_dummy_output(self, env):
        [x,shape] = self.node.input
        return env[x].reshape(list(env[shape]))

    def to_Edge(self, env, constants):
        [x,_] = self.node.input
        [y] = self.node.output
        return [
            Reshape(
                [x],
                list(self.node.output),
                shape=list(map(int, env[y].shape))
            )
        ]

class OpSoftmax(Op):

    def __init__(self, node):
        super().__init__(node)

        self.axis = 1
        for attr in self.node.attribute:
            if attr.name == 'axis':
                self.axis = attr.i

    def get_dummy_output(self, env):
        [x] = self.node.input
        return env[x]

    def to_Edge(self, env, constants):
        return [
            Softmax(
                list(self.node.input),
                list(self.node.output),
                axis=self.axis
            )
        ]

class OpSum(Op):

    def __init__(self, node):
        super().__init__(node)

    def get_dummy_output(self, env):
        xs = map(lambda x: env[x], self.node.input)
        return functools.reduce(lambda a,b: a+b, xs)

    def to_Edge(self, env, constants):
        if len(self.node.input) != 2:
            raise UnsupportedONNXOperation(self.node, '# of inputs must be 2')
        return [ Add(list(self.node.input), list(self.node.output)) ]

class OpTranspose(Op):

    def __init__(self, node):
        super().__init__(node)

        self.perm = None
        for attr in node.attribute:
            if attr.name == 'perm':
                self.perm = list(attr.ints)

    def get_dummy_output(self, env):
        [x] = self.node.input
        return np.transpose(env[x], self.perm)

    def to_Edge(self, env, constants):
        return [
            Transpose(
                list(self.node.input),
                list(self.node.output),
                axes=self.perm
            )
        ]

def evaluate(node, env):
    op_name = 'Op{}'.format(node.op_type)
    if hasattr(sys.modules[__name__], op_name):
        return getattr(sys.modules[__name__], op_name)(node).get_dummy_output(env)
    else:
        raise UnsupportedONNXOperation(node, 'not implemented')

class ONNX:
    def __init__(self, path):
        self.model = onnx.load(path)
        onnx.checker.check_model(self.model)
        self.sess = onnxruntime.InferenceSession(path)
        self.nodes = self._reconstruct_value_info()
        self.constant_nodes = self._eval_nodes(self._list_constant_nodes())

    def _reconstruct_value_info(self):
        outputs = list(map(lambda x: x.name, self.sess.get_outputs()))
        def dfs(visited, nodes, result):
            for n in nodes:
                _input = self._find_input(n)
                initializer = self._find_initializer(n)
                if initializer is not None:
                    result[n] = tensor_to_narray(initializer)
                elif _input is not None:
                    result[n] = to_dummy_input(_input)
                else:
                    generator = self._find_generator(n)
                    next_nodes = []
                    if hasattr(generator, 'input'):
                        next_nodes = [ i for i in generator.input if i not in visited ]
                    dfs(visited, next_nodes, result)
                    result[n] = evaluate(generator, result)
                visited.append(n)
        result = {}
        dfs([], outputs, result)
        return result

    def _find(self, p, xs, default=None):
        return next(filter(p, xs), default)

    def _find_initializer(self, name):
        return self._find(lambda n: name == n.name, self.model.graph.initializer)

    def _has_initializer(self, name):
        return self._find_initializer(name) is not None

    def _find_generator(self, name):
        return self._find(lambda n: name in n.output, self.model.graph.node)

    def _find_input(self, name):
        return self._find(lambda n: name == n.name, self.model.graph.input)

    def _has_input(self, name):
        return self._find_input(name) is not None

    def to_MLIR(self):
        inputs = list(map(lambda x: x.name, self.sess.get_inputs()))
        outputs = list(map(lambda x: x.name, self.sess.get_outputs()))
        edges = self._to_MLIR_edges()
        nodes = [ Node(n, self.nodes[n].dtype.str, list(self.nodes[n].shape)) for n in set(chain.from_iterable(map(lambda x: x.inputs + x.outputs, edges))) ]

        # rename to C ident (some frameworks don't satisfy the onnx spec.)
        renaming_table = dict([ (n.name, 'v{}'.format(i)) for i,n in enumerate(nodes) ])
        def rename(x):
            return renaming_table[x]
        inputs = list(map(rename, inputs))
        outputs = list(map(rename, outputs))
        def rename_edge(e):
            e.inputs = list(map(rename, e.inputs))
            e.outputs = list(map(rename, e.outputs))
            return e
        edges = list(map(rename_edge, edges))
        def rename_node(n):
            n.name = rename(n.name)
            return n
        nodes = list(map(rename_node, nodes))

        return MLIR(
            self.model.graph.name,
            self.model.producer_name,
            self.model.producer_version,
            inputs,
            outputs,
            nodes,
            edges
        )

    def _eval_nodes(self, nodes):
        m = copy.deepcopy(self.model)
        for n in m.graph.output:
            m.graph.output.remove(n)
        m.graph.output.extend(map(lambda n: narray_to_value_info(n, self.nodes[n]), nodes))
        onnx.save(m, '/tmp/tmp.onnx')
        dummy_sess = onnxruntime.InferenceSession('/tmp/tmp.onnx')
        inputs = dict([ (x.name, self.nodes[x.name]) for x in dummy_sess.get_inputs() ])
        output_names = list(map(lambda x: x.name, dummy_sess.get_outputs()))
        if output_names != []:
            result = dummy_sess.run(output_names, inputs)
        else:
            result = []
        return dict(zip(output_names, result))

    def test(self):
        m = copy.deepcopy(self.model)
        for n in m.graph.output:
            m.graph.output.remove(n)
        m.graph.output.extend(map(lambda nv: narray_to_value_info(nv[0], nv[1]), self.nodes.items()))
        for n in m.graph.input:
            m.graph.output.remove(n)
        onnx.save(m, '/tmp/test.onnx')

    def _to_MLIR_edges(self):
        outputs = list(map(lambda x: x.name, self.sess.get_outputs()))
        visited = []
        edges = []
        while outputs != []:
            o = outputs.pop(0)
            if o in visited:
                continue
            visited.append(o)
            generator = self._find_generator(o)
            if generator is not None:
                edge = self._from_operator_to_edge(generator)
                inputs = list(chain.from_iterable(map(lambda x: x.inputs, edge)))
                outputs += inputs
                edges += edge
            initializer = self._find_initializer(o)
            if initializer is not None:
                edge = self._from_initializer_to_edge(initializer)
                edges += edge
        return edges

    def _list_constant_nodes(self):
        outputs = list(map(lambda x: x.name, self.sess.get_outputs()))
        def dfs(visited, nodes, result):
            for n in nodes:
                if self._has_initializer(n):
                    result.append(n)
                elif self._has_input(n):
                    pass
                else:
                    generator = self._find_generator(n)
                    next_nodes = []
                    if hasattr(generator, 'input'):
                        next_nodes = [ i for i in generator.input if i not in visited ]
                    dfs(visited, next_nodes, result)
                    if hasattr(generator, 'input'):
                        if all([ i in result for i in generator.input ]):
                            for o in generator.output:
                                result.append(o)
                    else:
                        for o in generator.output:
                            result.append(o)
                visited.append(n)
        result = []
        dfs([], outputs, result)
        return result

    def _from_initializer_to_edge(self, node):
        return [
            Edge([], [node.name], Function('Constant', {b'value': encode_ndarray(self.nodes[node.name])}))
        ]

    def _from_operator_to_edge(self, node):
        op_name = 'Op{}'.format(node.op_type)
        if hasattr(sys.modules[__name__], op_name):
            return getattr(sys.modules[__name__], op_name)(node).to_Edge(self.nodes, self.constant_nodes)
        else:
            raise UnsupportedONNXOperation(node, 'not implemented')

def to_dummy_input(x):
    if hasattr(x.type, 'tensor_type'):
        if x.type.tensor_type.elem_type == onnx.TensorProto.FLOAT:
            return np.zeros(tuple(map(lambda d: d.dim_value, x.type.tensor_type.shape.dim)), dtype=np.float32)
        else:
            raise 'unsupported'
    else:
        raise 'unsupported'

def main(args):
    o = ONNX(args.input)
    # o.test()
    mlir = o.to_MLIR()
    if False:
        import pprint
        pp = pprint.PrettyPrinter(indent=4)
        pp.pprint(mlir.to_dict())
    mlir.dump(args.output)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='ONNX to MLIR Converter')
    parser.add_argument('-o', '--output', dest='output', type=str, required=True,
                        metavar='MLIR', help='MLIR file path')
    parser.add_argument(dest='input', type=str,
                        metavar='ONNX', help='ONNX file path')
    main(parser.parse_args())
