#!/usr/bin/env python3
import io
import copy
import functools
import numpy as np
import argparse
import onnx
import onnxruntime
import msgpack
from itertools import chain

class MLIR:
    def __init__(self, version = 0, model = None):
        self.version = version
        self.model = model

    def to_dict(self):
        return {
            b'mlir': {
                b'version': self.version,
                b'model': self.model.to_dict()
            }
        }

    def save(self, path):
        with open(path, 'w') as f:
            f.buffer.write(msgpack.packb(self.to_dict()))

class Model:

    def __init__(self, name, generator, inputs, outputs, nodes, edges):
        self.name = name
        self.generator = generator
        self.inputs = inputs
        self.outputs = outputs
        self.nodes = nodes
        self.edges = edges

    def to_dict(self):
        return {
            b'name': self.name,
            b'generator': self.generator,
            b'inputs': self.inputs,
            b'outputs': self.outputs,
            b'nodes': [ n.to_dict() for n in self.nodes ],
            b'edges': [ e.to_dict() for e in self.edges ]
        }

class Node:

    def __init__(self, name, arr):
        self.name = name
        self.arr = arr

    def to_dict(self):
        return {
            b'name': self.name,
            b'dtype': self.arr.dtype.str,
            b'shape': list(self.arr.shape)
        }

class Edge:

    def __init__(self, inputs, outputs, function):
        self.inputs = list(inputs)
        self.outputs = list(outputs)
        self.function = function

    def to_dict(self):
        return {
            b'inputs': self.inputs,
            b'outputs': self.outputs,
            **self.function.to_dict()
        }

    def __str__(self):
        return '{} ({} -> {})'.format(self.function, self.inputs, self.outputs)

class Function:

    def __init__(self, name, params = {}):
        self.name = name
        self.params = params

    def to_dict(self):
        return {
            b'name': self.name,
            b'params': self.params
        }

    def __str__(self):
        return '{}'.format(self.name)

def tensor_to_narray(tensor):
    dtypes = {
        onnx.TensorProto.BOOL: np.bool,
        onnx.TensorProto.INT8: np.int8,
        onnx.TensorProto.INT16: np.int16,
        onnx.TensorProto.INT32: np.int32,
        onnx.TensorProto.INT64: np.int64,
        onnx.TensorProto.UINT8: np.uint8,
        onnx.TensorProto.UINT16: np.uint16,
        onnx.TensorProto.UINT32: np.uint32,
        onnx.TensorProto.UINT64: np.uint64,
        onnx.TensorProto.FLOAT16: np.float16,
        onnx.TensorProto.FLOAT: np.float32,
        onnx.TensorProto.DOUBLE: np.float64,
    }
    arr = []
    if tensor.data_type in [onnx.TensorProto.FLOAT]:
        arr = tensor.float_data
    elif tensor.data_type in [onnx.TensorProto.INT32, onnx.TensorProto.INT16, onnx.TensorProto.INT8, onnx.TensorProto.UINT16, onnx.TensorProto.UINT8, onnx.TensorProto.BOOL, onnx.TensorProto.FLOAT16]:
        arr = tensor.int32_data
    elif tensor.data_type in [onnx.TensorProto.INT64]:
        arr = tensor.int64_data
    elif tensor.data_type in [onnx.TensorProto.DOUBLE]:
        arr = tensor.double_data
    elif tensor.data_type in [onnx.TensorProto.UINT32, onnx.TensorProto.UINT64]:
        arr = tensor.uint64_data
    else:
        raise 'unsupported'
    if arr == []:
        result = np.frombuffer(tensor.raw_data, dtype=dtypes[tensor.data_type])
    else:
        result = np.array(arr, dtype=dtypes[tensor.data_type])
    shape = tensor.dims if tensor.dims != [] else [1]
    return result.reshape(*shape)

def narray_to_value_info(name, arr):
    dtypes = {
        'bool': onnx.TensorProto.BOOL,
        'int8': onnx.TensorProto.INT8,
        'int16': onnx.TensorProto.INT16,
        'int32': onnx.TensorProto.INT32,
        'int64': onnx.TensorProto.INT64,
        'uint8': onnx.TensorProto.UINT8,
        'uint16': onnx.TensorProto.UINT16,
        'uint32': onnx.TensorProto.UINT32,
        'uint64': onnx.TensorProto.UINT64,
        'float16': onnx.TensorProto.FLOAT16,
        'float32': onnx.TensorProto.FLOAT,
        'float64': onnx.TensorProto.DOUBLE,
    }
    return onnx.helper.make_tensor_value_info(name, dtypes[str(arr.dtype)], arr.shape)

def encode_ndarray(obj):
    if obj is None:
        return None
    else:
        with io.BytesIO() as out:
            np.save(out, obj.copy())
            return { b'ndarray': out.getvalue() }

def auto_pad_to_manual_pad(n, k, s, auto_pad):
    if n % s == 0:
        pad = max(k - s, 0)
    else:
        pad = max(k - n % s, 0)
    if auto_pad == b'SAME_LOWER':
        pad_before = pad // 2
        pad_after  = pad - pad_before
        return (pad_before, pad_after)
    elif auto_pad == b'SAME_UPPER':
        pad_after = pad // 2
        pad_before  = pad - pad_after
        return (pad_before, pad_after)
    elif auto_pad == b'VALID':
        return (0, 0)
    else:
        raise 'invalid'

def from_graph_to_inputs(graph):
    initializers = [ i.name for i in graph.initializer ]
    return [ i.name for i in graph.input if i.name not in initializers ]

def from_graph_to_outputs(graph):
    return [ o.name for o in graph.output ]

class OpAdd:
    def __init__(self, node):
        self.node = node

    def get_dummy_output(self, env):
        [a,b] = self.node.input
        return env[a] + env[b]

class OpAveragePool(Function):
    def __init__(self, node):
        self.node = node

        self.kernel_shape = None
        self.auto_pad = b'NOTSET'
        self.pads = None
        self.storage_order = 0
        self.strides = (1,1)
        for attr in node.attribute:
            if attr.name == 'kernel_shape':
                self.kernel_shape = attr.ints
            if attr.name == 'storage_order':
                self.storage_order = attr.i
            if attr.name == 'strides':
                self.strides = attr.ints
            if attr.name == 'auto_pad':
                self.auto_pad = attr.s
            if attr.name == 'pads':
                self.pads = attr.ints

    def get_dummy_output(self, env):
        [x] = self.node.input

        _input = env[x]
        batch = _input.shape[0]
        channel = _input.shape[1]
        in_h = _input.shape[2]
        in_w = _input.shape[3]
        kh = self.kernel_shape[0]
        kw = self.kernel_shape[1]
        sy = self.strides[0]
        sx = self.strides[1]

        if self.auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if self.pads is not None:
                pad_h = (self.pads[0], self.pads[2])
                pad_w = (self.pads[1], self.pads[3])
        else:
            pad_h = auto_pad_to_manual_pad(in_h, kh, sy, self.auto_pad)
            pad_w = auto_pad_to_manual_pad(in_w, kw, sx, self.auto_pad)

        out_h = ((pad_h[0] + in_h + pad_h[1]) - ((kh - 1) * 1 + 1)) // sy + 1
        out_w = ((pad_w[0] + in_w + pad_w[1]) - ((kw - 1) * 1 + 1)) // sx + 1

        return np.zeros((batch, channel, out_h, out_w), dtype=env[x].dtype)

class OpBatchNormalization(Function):
    def __init__(self, node):
        self.node = node

    def get_dummy_output(self, env):
        [x,_,_,_,_] = self.node.input
        return env[x]

class OpConv(Function):
    def __init__(self, node):
        self.node = node

        self.kernel_shape = None
        self.auto_pad = b'NOTSET'
        self.pads = None
        self.strides = (1,1)
        self.dilations = (1,1)
        self.group = 1
        for attr in node.attribute:
            if attr.name == 'kernel_shape':
                self.kernel_shape = attr.ints
            if attr.name == 'dilations':
                self.dilations = attr.ints
            if attr.name == 'group':
                self.group = attr.i
            if attr.name == 'strides':
                self.strides = attr.ints
            if attr.name == 'auto_pad':
                self.auto_pad = attr.s
            if attr.name == 'pads':
                self.pads = attr.ints

    def get_dummy_output(self, env):

        if len(self.node.input) == 3:
            [x,W,b] = self.node.input
        else:
            [x,W] = self.node.input
            b = None

        _input = env[x]
        batch = _input.shape[0]
        in_ch = _input.shape[1]
        in_h = _input.shape[2]
        in_w = _input.shape[3]
        kh = self.kernel_shape[0]
        kw = self.kernel_shape[1]
        sy = self.strides[0]
        sx = self.strides[1]
        dy = self.dilations[0]
        dx = self.dilations[1]

        if self.auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if self.pads is not None:
                pad_h = (self.pads[0], self.pads[2])
                pad_w = (self.pads[1], self.pads[3])
        else:
            pad_h = auto_pad_to_manual_pad(in_h, kh, sy, self.auto_pad)
            pad_w = auto_pad_to_manual_pad(in_w, kw, sx, self.auto_pad)

        out_ch = env[W].shape[0]

        out_h = ((pad_h[0] + in_h + pad_h[1]) - ((kh - 1) * dy + 1)) // sy + 1
        out_w = ((pad_w[0] + in_w + pad_w[1]) - ((kw - 1) * dx + 1)) // sx + 1

        return np.zeros((batch, out_ch, out_h, out_w), dtype=env[x].dtype)

class OpGemm(Function):
    def __init__(self, node):
        self.node = node

        self.alpha = 1.0
        self.beta = 1.0
        self.transA = 0
        self.transB = 0
        for attr in node.attribute:
            if attr.name == 'alpha':
                self.alpha = attr.i
            if attr.name == 'beta':
                self.beta = attr.i
            if attr.name == 'transA':
                self.transA = attr.i
            if attr.name == 'transB':
                self.transB = attr.i

    def get_dummy_output(self, env):
        [A,B,C] = self.node.input
        a = env[A]
        if self.transA == 1:
            a = a.T
        b = env[B]
        if self.transB == 1:
            b = b.T
        return a.dot(b)+env[C]

class OpMatMul(Function):
    def __init__(self, node):
        self.node = node

    def get_dummy_output(self, env):
        [x,W] = self.node.input
        return env[x].dot(env[W])

class OpMaxPool(Function):
    def __init__(self, node):
        self.node = node

        self.kernel_shape = None
        self.auto_pad = b'NOTSET'
        self.pads = None
        self.storage_order = 0
        self.strides = (1,1)
        for attr in node.attribute:
            if attr.name == 'kernel_shape':
                self.kernel_shape = attr.ints
            if attr.name == 'storage_order':
                self.storage_order = attr.i
            if attr.name == 'strides':
                self.strides = attr.ints
            if attr.name == 'auto_pad':
                self.auto_pad = attr.s
            if attr.name == 'pads':
                self.pads = attr.ints

    def get_dummy_output(self, env):
        [x] = self.node.input

        _input = env[x]
        batch = _input.shape[0]
        channel = _input.shape[1]
        in_h = _input.shape[2]
        in_w = _input.shape[3]
        kh = self.kernel_shape[0]
        kw = self.kernel_shape[1]
        sy = self.strides[0]
        sx = self.strides[1]

        if self.auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if self.pads is not None:
                pad_h = (self.pads[0], self.pads[2])
                pad_w = (self.pads[1], self.pads[3])
        else:
            pad_h = auto_pad_to_manual_pad(in_h, kh, sy, self.auto_pad)
            pad_w = auto_pad_to_manual_pad(in_w, kw, sx, self.auto_pad)

        out_h = ((pad_h[0] + in_h + pad_h[1]) - ((kh - 1) * 1 + 1)) // sy + 1
        out_w = ((pad_w[0] + in_w + pad_w[1]) - ((kw - 1) * 1 + 1)) // sx + 1

        return np.zeros((batch, channel, out_h, out_w), dtype=env[x].dtype)

class OpRelu(Function):
    def __init__(self, node):
        self.node = node

    def get_dummy_output(self, env):
        [x] = self.node.input
        return env[x]

class OpReshape(Function):
    def __init__(self, node):
        self.node = node

    def get_dummy_output(self, env):
        [x,shape] = self.node.input
        return env[x].reshape(list(env[shape]))

class OpSoftmax(Function):
    def __init__(self, node):
        self.node = node

    def get_dummy_output(self, env):
        [x] = self.node.input
        return env[x]

class OpSum(Function):
    def __init__(self, node):
        self.node = node

    def get_dummy_output(self, env):
        xs = map(lambda x: env[x], self.node.input)
        return functools.reduce(lambda a,b: a+b, xs)

_OP_CLASSES = {
    'Add': OpAdd,
    'AveragePool': OpAveragePool,
    'BatchNormalization': OpBatchNormalization,
    'Conv': OpConv,
    'Gemm': OpGemm,
    'MatMul': OpMatMul,
    'MaxPool': OpMaxPool,
    'Relu': OpRelu,
    'Reshape': OpReshape,
    'Softmax': OpSoftmax,
    'Sum': OpSum,
}

def evaluate(node, env):
    return _OP_CLASSES[node.op_type](node).get_dummy_output(env)

class ONNX:
    def __init__(self, path):
        self.model = onnx.load(path)
        onnx.checker.check_model(self.model)
        self.nodes = self._reconstruct_value_info()
        self.constant_nodes = self._eval_nodes(self._list_constant_nodes())

    def _reconstruct_value_info(self):
        outputs = from_graph_to_outputs(self.model.graph)
        def dfs(visited, nodes, result):
            for n in nodes:
                _input = self._find_input(n)
                initializer = self._find_initializer(n)
                if initializer is not None:
                    result[n] = tensor_to_narray(initializer)
                elif _input is not None:
                    result[n] = to_dummy_input(_input)
                else:
                    generator = self._find_generator(n)
                    next_nodes = []
                    if hasattr(generator, 'input'):
                        next_nodes = [ i for i in generator.input if i not in visited ]
                    dfs(visited, next_nodes, result)
                    result[n] = evaluate(generator, result)
                visited.append(n)
        result = {}
        dfs([], outputs, result)
        return result

    def _find(self, p, xs, default=None):
        return next(filter(p, xs), default)

    def _find_initializer(self, name):
        return self._find(lambda n: name == n.name, self.model.graph.initializer)

    def _has_initializer(self, name):
        return self._find_initializer(name) is not None

    def _find_generator(self, name):
        return self._find(lambda n: name in n.output, self.model.graph.node)

    def _has_generator(self, name):
        return self._find_generator(name) is not None

    def _find_input(self, name):
        return self._find(lambda n: name == n.name, self.model.graph.input)

    def _has_input(self, name):
        return self._find_input(name) is not None

    def _has_constant(self, name):
        return self._get_constant(name) is not None

    def _get_constant(self, name):
        i = self._find_initializer(name)
        if i is not None:
            return tensor_to_narray(i)
        elif name in self.constant_nodes:
            return self.constant_nodes[name]
        else:
            return None

    def _find_node(self, name):
        return self.nodes[name] # self._find(lambda n: name == n.name, self.model.graph.value_info)

    def to_MLIR(self):
        inputs = from_graph_to_inputs(self.model.graph)
        outputs = from_graph_to_outputs(self.model.graph)
        edges = self._to_MLIR_edges()
        nodes = [ Node(n, self.nodes[n]) for n in set(chain.from_iterable(map(lambda x: x.inputs + x.outputs, edges))) ]

        # rename to C ident (some frameworks don't satisfy the onnx spec.)
        renaming_table = dict([ (n.name, 'v{}'.format(i)) for i,n in enumerate(nodes) ])
        def rename(x):
            return renaming_table[x]
        inputs = list(map(rename, inputs))
        outputs = list(map(rename, outputs))
        def rename_edge(e):
            e.inputs = list(map(rename, e.inputs))
            e.outputs = list(map(rename, e.outputs))
            return e
        edges = list(map(rename_edge, edges))
        def rename_node(n):
            n.name = rename(n.name)
            return n
        nodes = list(map(rename_node, nodes))

        return MLIR(
            model = Model(
                self.model.graph.name,
                {b'name': self.model.producer_name, b'version': self.model.producer_version},
                inputs,
                outputs,
                nodes,
                edges
            ),
        )

    def _eval_nodes(self, nodes):
        m = copy.deepcopy(self.model)
        for n in m.graph.output:
            m.graph.output.remove(n)
        m.graph.output.extend(map(lambda n: narray_to_value_info(n, self.nodes[n]), nodes))
        input_names = from_graph_to_inputs(self.model.graph)
        output_names = from_graph_to_outputs(self.model.graph)
        initializers = list(map(lambda x: x.name, self.model.graph.initializer))
        inputs = filter(lambda x: x not in initializers, input_names)
        inputs = map(lambda x: list(filter(lambda y: y.name == x, self.model.graph.input))[0], inputs)
        inputs = list(map(to_dummy_input, inputs))
        inputs = dict(zip(input_names, inputs))
        onnx.save(m, '/tmp/tmp.onnx')
        sess = onnxruntime.InferenceSession('/tmp/tmp.onnx')
        output_names = [ x.name for x in sess.get_outputs() ]
        if output_names != []:
            result = sess.run(output_names, inputs)
        else:
            result = []
        return dict(zip(output_names, result))

    def _to_MLIR_edges(self):
        outputs = from_graph_to_outputs(self.model.graph)
        visited = []
        edges = []
        while outputs != []:
            o = outputs.pop(0)
            if o in visited:
                continue
            visited.append(o)
            generator = self._find_generator(o)
            if generator is not None:
                edge = self._from_operator_to_edge(generator)
                inputs = list(chain.from_iterable(map(lambda x: x.inputs, edge)))
                outputs += inputs
                edges += edge
            initializer = self._find_initializer(o)
            if initializer is not None:
                edge = self._from_initializer_to_edge(initializer)
                edges += edge
        return edges

    def _list_constant_nodes(self):
        outputs = from_graph_to_outputs(self.model.graph)
        def dfs(visited, nodes, result):
            for n in nodes:
                if self._has_initializer(n):
                    result.append(n)
                elif self._has_input(n):
                    pass
                else:
                    generator = self._find_generator(n)
                    next_nodes = []
                    if hasattr(generator, 'input'):
                        next_nodes = [ i for i in generator.input if i not in visited ]
                    dfs(visited, next_nodes, result)
                    if hasattr(generator, 'input'):
                        if all([ i in result for i in generator.input ]):
                            for o in generator.output:
                                result.append(o)
                    else:
                        for o in generator.output:
                            result.append(o)
                visited.append(n)
        result = []
        dfs([], outputs, result)
        return result

    def op2edge_Add(self, node):
        [a, b] = node.input
        ca = self._get_constant(a)
        cb = self._get_constant(b)
        if ca is not None and cb is None:
            return [
                Edge([b], node.output, Function('Bias', {
                    b'axis': 0,
                    b'b': encode_ndarray(ca)
                }))
            ]
        elif ca is None and cb is not None:
            return [
                Edge([a], node.output, Function('Bias', {
                    b'axis': 0,
                    b'b': encode_ndarray(cb)
                    }))
            ]
        elif ca is None and cb is None:
            return [
                Edge(node.input, node.output, Function('Add'))
            ]
        else:
            raise 'unsupported'

    def op2edge_AveragePool(self, node):
        auto_pad = b'NOTSET'
        pads = None
        for attr in node.attribute:
            if attr.name == 'kernel_shape':
                [kh, kw] = attr.ints
            if attr.name == 'strides':
                strides = attr.ints
                [sy, sx] = strides
            if attr.name == 'auto_pad':
                auto_pad = attr.s
            if attr.name == 'pads':
                pads = attr.ints
        if auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if pads is not None:
                pad_h = (pads[0], pads[2])
                pad_w = (pads[1], pads[3])
        else:
            i = self._find_node(x)
            h = i.shape[2]
            w = i.shape[3]
            pad_h = auto_pad_to_manual_pad(h, kh, sy, auto_pad)
            pad_w = auto_pad_to_manual_pad(w, kw, sx, auto_pad)
        return [
            Edge(node.input, node.output, Function('AveragePooling2D', {
                b'kernel': (kh, kw),
                b'stride': tuple(strides),
                b'pad_h': pad_h,
                b'pad_w': pad_w,
            }))
        ]

    def op2edge_BatchNormalization(self, node):
        [x,gamma,beta,mean,var] = node.input
        gamma = self._get_constant(gamma)
        if gamma is None:
            raise 'unsupported'
        beta = self._get_constant(beta)
        if beta is None:
            raise 'unsupported'
        mean = self._get_constant(mean)
        if mean is None:
            raise 'unsupported'
        var = self._get_constant(var)
        if var is None:
            raise 'unsupported'
        eps = 1e-05
        for attr in node.attribute:
            if attr.name == 'epsilon':
                eps = attr.f
        return [
            Edge([x], node.output, Function('BatchNormalization', {
                b'eps': eps,
                b'avg_mean': encode_ndarray(mean),
                b'avg_var': encode_ndarray(var),
                b'gamma': encode_ndarray(gamma),
                b'beta': encode_ndarray(beta),
            }))
        ]

    def op2edge_Conv(self, node):
        b = None
        if len(node.input) == 2:
            [x, W] = node.input
        elif len(node.input) == 3:
            [x, W, b] = node.input
        else:
            raise 'invalid'
        W = self._get_constant(W)
        if W is None:
            raise 'unsupported'
        if b is not None:
            b = self._get_constant(b)
            if b is None:
                raise 'unsupported'
        kh = W.shape[2]
        kw = W.shape[3]
        auto_pad = b'NOTSET'
        pads = None
        strides = (1,1)
        dilations = (1,1)
        group = 1
        for attr in node.attribute:
            if attr.name == 'dilations':
                dilations = attr.ints
            if attr.name == 'group':
                group = attr.i
            if attr.name == 'strides':
                strides = attr.ints
                [sy, sx] = strides
            if attr.name == 'auto_pad':
                auto_pad = attr.s
            if attr.name == 'pads':
                pads = attr.ints
        if auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if pads is not None:
                pad_h = (pads[0], pads[2])
                pad_w = (pads[1], pads[3])
        else:
            i = self._find_node(x)
            h = i.shape[2]
            w = i.shape[3]
            pad_h = auto_pad_to_manual_pad(h, kh, sy, auto_pad)
            pad_w = auto_pad_to_manual_pad(w, kw, sx, auto_pad)
        return [
            Edge([x], node.output, Function('Convolution2D', {
                b'W': encode_ndarray(W),
                b'b': encode_ndarray(b),
                b'stride': tuple(strides),
                b'pad_h': pad_h,
                b'pad_w': pad_w,
                b'dilate': tuple(dilations),
                b'groups': int(group)
            }))
        ]

    def op2edge_Gemm(self, node):
        [A,B,C] = node.input
        B = self._get_constant(B)
        if B is None:
            raise 'unsupported'
        C = self._get_constant(C)
        if C is None:
            raise 'unsupported'
        alpha = 1.0
        beta = 1.0
        transA = 0
        transB = 0
        for attr in node.attribute:
            if attr.name == 'alpha':
                alpha = attr.i
            if attr.name == 'beta':
                beta = attr.i
            if attr.name == 'transA':
                transA = attr.i
            if attr.name == 'transB':
                transB = attr.i
        if len(C.shape) == 2 and C.shape[0] != 1:
            raise 'unsupported'
        return [
            Edge([A], node.output, Function('Linear', {
                b'W': encode_ndarray(alpha * B),
                b'b': encode_ndarray(beta * C.ravel())
                }))
            ]

    def op2edge_MatMul(self, node):
        [x, W] = node.input
        W = self._get_constant(W)
        if W is not None:
            return [
                Edge([x], node.output, Function('Linear', {
                    b'W': encode_ndarray(W),
                    b'b': None
                }))
            ]
        else:
            raise 'unsupported'

    def op2edge_MaxPool(self, node):
        auto_pad = b'NOTSET'
        pads = None
        for attr in node.attribute:
            if attr.name == 'kernel_shape':
                [kh, kw] = attr.ints
            if attr.name == 'strides':
                strides = attr.ints
                [sy, sx] = strides
            if attr.name == 'auto_pad':
                auto_pad = attr.s
            if attr.name == 'pads':
                pads = attr.ints
        if auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if pads is not None:
                pad_h = (pads[0], pads[2])
                pad_w = (pads[1], pads[3])
        else:
            i = self._find_node(x)
            h = i.shape[2]
            w = i.shape[3]
            pad_h = auto_pad_to_manual_pad(h, kh, sy, auto_pad)
            pad_w = auto_pad_to_manual_pad(w, kw, sx, auto_pad)
        return [
            Edge(node.input, node.output, Function('MaxPooling2D', {
                b'kernel': (kh, kw),
                b'stride': tuple(strides),
                b'pad_h': pad_h,
                b'pad_w': pad_w,
            }))
        ]

    def op2edge_Relu(self, node):
        return [
            Edge(node.input, node.output, Function('ReLU'))
        ]

    def op2edge_Reshape(self, node):
        [x, shape] = node.input
        shape = self._find_initializer(shape)
        return [
            Edge([x], node.output, Function('Reshape', {b'shape': list(shape.int64_data)}))
        ]

    def op2edge_Softmax(self, node):
        axis = 1
        for attr in node.attribute:
            if attr.name == 'axis':
                axis = attr.i
        return [
            Edge(node.input, node.output, Function('Softmax', {b'axis': axis}))
        ]

    def op2edge_Sum(self, node):
        if len(node.input) != 2:
            raise 'unsupported'
        return [
            Edge(node.input, node.output, Function('Add'))
        ]

    def _from_initializer_to_edge(self, node):
        return [
            Edge([], [node.name], Function('Constant', {b'value': encode_ndarray(self._get_constant(node.name))}))
        ]

    def _from_operator_to_edge(self, node):
        if all(map(lambda x: self._has_constant(x), node.output)):
            return [
                Edge([], [x], Function('Constant', {b'value': encode_ndarray(self._get_constant(x))}))
                for x in node.output
            ]
        else:
            return {
                'Add': self.op2edge_Add,
                'AveragePool': self.op2edge_AveragePool,
                'BatchNormalization': self.op2edge_BatchNormalization,
                'Conv': self.op2edge_Conv,
                'Gemm': self.op2edge_Gemm,
                'MatMul': self.op2edge_MatMul,
                'MaxPool': self.op2edge_MaxPool,
                'Relu': self.op2edge_Relu,
                'Reshape': self.op2edge_Reshape,
                'Softmax': self.op2edge_Softmax,
                'Sum': self.op2edge_Sum,
            }[node.op_type](node)

def to_dummy_input(x):
    if hasattr(x.type, 'tensor_type'):
        if x.type.tensor_type.elem_type == onnx.TensorProto.FLOAT:
            return np.zeros(tuple(map(lambda d: d.dim_value, x.type.tensor_type.shape.dim)), dtype=np.float32)
        else:
            'unsupported'
    else:
        raise 'unsupported'

def main(args):
    mlir = ONNX(args.input).to_MLIR()
    mlir.save(args.output)
    #import pprint
    #pp = pprint.PrettyPrinter(indent=4)
    #pp.pprint(mlir.to_dict())

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='ONNX to MLIR Converter')
    parser.add_argument('-o', '--output', dest='output', type=str, required=True,
                        metavar='MLIR', help='MLIR file path')
    parser.add_argument(dest='input', type=str,
                        metavar='ONNX', help='ONNX file path')
    main(parser.parse_args())
