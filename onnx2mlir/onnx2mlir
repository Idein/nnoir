#!/usr/bin/env python3
import io
import sys
import copy
import functools
import numpy as np
import argparse
import onnx
import onnxruntime
import msgpack
from itertools import chain

class MLIR:
    def __init__(self, version = 0, model = None):
        self.version = version
        self.model = model

    def to_dict(self):
        return {
            b'mlir': {
                b'version': self.version,
                b'model': self.model.to_dict()
            }
        }

    def save(self, path):
        with open(path, 'w') as f:
            f.buffer.write(msgpack.packb(self.to_dict()))

class Model:

    def __init__(self, name, generator, inputs, outputs, nodes, edges):
        self.name = name
        self.generator = generator
        self.inputs = inputs
        self.outputs = outputs
        self.nodes = nodes
        self.edges = edges

    def to_dict(self):
        return {
            b'name': self.name,
            b'generator': self.generator,
            b'inputs': self.inputs,
            b'outputs': self.outputs,
            b'nodes': [ n.to_dict() for n in self.nodes ],
            b'edges': [ e.to_dict() for e in self.edges ]
        }

class Node:

    def __init__(self, name, arr):
        self.name = name
        self.arr = arr

    def to_dict(self):
        return {
            b'name': self.name,
            b'dtype': self.arr.dtype.str,
            b'shape': list(self.arr.shape)
        }

class Edge:

    def __init__(self, inputs, outputs, function):
        self.inputs = list(inputs)
        self.outputs = list(outputs)
        self.function = function

    def to_dict(self):
        return {
            b'inputs': self.inputs,
            b'outputs': self.outputs,
            **self.function.to_dict()
        }

    def __str__(self):
        return '{} ({} -> {})'.format(self.function, self.inputs, self.outputs)

class Function:

    def __init__(self, name, params = {}):
        self.name = name
        self.params = params

    def to_dict(self):
        return {
            b'name': self.name,
            b'params': self.params
        }

    def __str__(self):
        return '{}'.format(self.name)

def tensor_to_narray(tensor):
    arr = []
    storage = onnx.mapping.TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE[tensor.data_type]
    storage = onnx.mapping.STORAGE_TENSOR_TYPE_TO_FIELD[storage]
    arr = getattr(tensor, storage)
    if arr == []:
        result = np.frombuffer(tensor.raw_data, dtype=onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[tensor.data_type])
    else:
        result = np.array(arr, dtype=onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[tensor.data_type])
    shape = tensor.dims if tensor.dims != [] else [1]
    return result.reshape(*shape)

def narray_to_value_info(name, arr):
    return onnx.helper.make_tensor_value_info(name, onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[arr.dtype], arr.shape)

def encode_ndarray(obj):
    if obj is None:
        return None
    else:
        with io.BytesIO() as out:
            np.save(out, obj.copy())
            return { b'ndarray': out.getvalue() }

def auto_pad_to_manual_pad(n, k, s, d, auto_pad):
    dk = (k - 1) * d + 1
    if n % s == 0:
        pad = max(dk - s, 0)
    else:
        pad = max(dk - n % s, 0)
    if auto_pad == b'SAME_LOWER':
        pad_before = pad // 2
        pad_after  = pad - pad_before
        return (pad_before, pad_after)
    elif auto_pad == b'SAME_UPPER':
        pad_after = pad // 2
        pad_before  = pad - pad_after
        return (pad_before, pad_after)
    elif auto_pad == b'VALID':
        return (0, 0)
    else:
        raise 'invalid'

class Op:
    def __init__(self, node):
        self.node = node

    def get_dummy_output(self, env):
        raise 'unsupported'

    def to_Edge(self, env, constants):
        raise 'unsupported'

class OpAdd(Op):

    def __init__(self, node):
        super().__init__(node)

    def get_dummy_output(self, env):
        [a,b] = self.node.input
        return env[a] + env[b]

    def to_Edge(self, env, constants):
        [a, b] = self.node.input
        if a in constants and b not in constants:
            return [
                Edge([b], self.node.output, Function('Bias', {
                    b'axis': 0,
                    b'b': encode_ndarray(constants[a])
                }))
            ]
        elif a not in constants and b in constants:
            return [
                Edge([a], self.node.output, Function('Bias', {
                    b'axis': 0,
                    b'b': encode_ndarray(constants[b])
                    }))
            ]
        elif a not in constants and b not in constants:
            return [
                Edge(self.node.input, self.node.output, Function('Add'))
            ]
        else:
            raise 'bug! (unreachable here)'

class OpAveragePool(Op):

    def __init__(self, node):
        super().__init__(node)

        self.kernel_shape = None
        self.auto_pad = b'NOTSET'
        self.pads = None
        self.storage_order = 0
        self.strides = (1,1)
        for attr in node.attribute:
            if attr.name == 'kernel_shape':
                self.kernel_shape = attr.ints
            if attr.name == 'storage_order':
                self.storage_order = attr.i
            if attr.name == 'strides':
                self.strides = attr.ints
            if attr.name == 'auto_pad':
                self.auto_pad = attr.s
            if attr.name == 'pads':
                self.pads = attr.ints

    def get_dummy_output(self, env):
        [x] = self.node.input

        _input = env[x]
        batch = _input.shape[0]
        channel = _input.shape[1]
        in_h = _input.shape[2]
        in_w = _input.shape[3]
        kh = self.kernel_shape[0]
        kw = self.kernel_shape[1]
        sy = self.strides[0]
        sx = self.strides[1]

        if self.auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if self.pads is not None:
                pad_h = (self.pads[0], self.pads[2])
                pad_w = (self.pads[1], self.pads[3])
        else:
            pad_h = auto_pad_to_manual_pad(in_h, kh, sy, 1, self.auto_pad)
            pad_w = auto_pad_to_manual_pad(in_w, kw, sx, 1, self.auto_pad)

        out_h = ((pad_h[0] + in_h + pad_h[1]) - ((kh - 1) * 1 + 1)) // sy + 1
        out_w = ((pad_w[0] + in_w + pad_w[1]) - ((kw - 1) * 1 + 1)) // sx + 1

        return np.zeros((batch, channel, out_h, out_w), dtype=env[x].dtype)

    def to_Edge(self, env, constants):
        [x] = self.node.input

        _input = env[x]
        batch = _input.shape[0]
        channel = _input.shape[1]
        in_h = _input.shape[2]
        in_w = _input.shape[3]
        kh = self.kernel_shape[0]
        kw = self.kernel_shape[1]
        sy = self.strides[0]
        sx = self.strides[1]

        if self.auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if self.pads is not None:
                pad_h = (self.pads[0], self.pads[2])
                pad_w = (self.pads[1], self.pads[3])
        else:
            pad_h = auto_pad_to_manual_pad(in_h, kh, sy, 1, self.auto_pad)
            pad_w = auto_pad_to_manual_pad(in_w, kw, sx, 1, self.auto_pad)

        return [
            Edge(self.node.input, self.node.output, Function('AveragePooling2D', {
                b'kernel': (kh, kw),
                b'stride': (sy, sx),
                b'pad_h': pad_h,
                b'pad_w': pad_w,
            }))
        ]

class OpBatchNormalization(Op):
    def __init__(self, node):
        super().__init__(node)

    def get_dummy_output(self, env):
        [x,_,_,_,_] = self.node.input
        return env[x]

    def to_Edge(self, env, constants):
        [x,gamma,beta,mean,var] = self.node.input
        if gamma not in constants:
            raise 'unsupported'
        if beta not in constants:
            raise 'unsupported'
        if mean not in constants:
            raise 'unsupported'
        if var not in constants:
            raise 'unsupported'
        eps = 1e-05
        for attr in self.node.attribute:
            if attr.name == 'epsilon':
                eps = attr.f
        return [
            Edge([x], self.node.output, Function('BatchNormalization', {
                b'eps': eps,
                b'avg_mean': encode_ndarray(constants[mean]),
                b'avg_var': encode_ndarray(constants[var]),
                b'gamma': encode_ndarray(constants[gamma]),
                b'beta': encode_ndarray(constants[beta]),
            }))
        ]

class OpClip(Op):

    def __init__(self, node):
        super().__init__(node)

    def get_dummy_output(self, env):
        [x] = self.node.input
        return env[x]

    def to_Edge(self, env, constants):
        _min = -3.4028234663852886e+38
        _max =  3.4028234663852886e+38
        for attr in self.node.attribute:
            if attr.name == 'max':
                _max = attr.f
            if attr.name == 'min':
                _min = attr.f

        if _min != 0.0:
            raise 'unsupported'

        return [
            Edge(self.node.input, self.node.output, Function('ClippedReLU', {
                b'upper': _max,
            }))
        ]

class OpConcat(Op):

    def __init__(self, node):
        super().__init__(node)

        self.axis = None
        for attr in node.attribute:
            if attr.name == 'axis':
                self.axis = attr.i

    def get_dummy_output(self, env):
        return np.concatenate(list(map(lambda x: env[x], self.node.input)), axis=self.axis)

    def to_Edge(self, env, constants):
        return [
            Edge(self.node.input, self.node.output, Function('Concat', {
                b'axis': self.axis
            }))
        ]

class OpConv(Op):

    def __init__(self, node):
        super().__init__(node)

        self.kernel_shape = None
        self.auto_pad = b'NOTSET'
        self.pads = None
        self.strides = (1,1)
        self.dilations = (1,1)
        self.group = 1
        for attr in node.attribute:
            if attr.name == 'kernel_shape':
                self.kernel_shape = attr.ints
            if attr.name == 'dilations':
                self.dilations = attr.ints
            if attr.name == 'group':
                self.group = attr.i
            if attr.name == 'strides':
                self.strides = attr.ints
            if attr.name == 'auto_pad':
                self.auto_pad = attr.s
            if attr.name == 'pads':
                self.pads = attr.ints

    def get_dummy_output(self, env):

        if len(self.node.input) == 3:
            [x,W,b] = self.node.input
        else:
            [x,W] = self.node.input
            b = None

        _input = env[x]
        batch = _input.shape[0]
        in_ch = _input.shape[1]
        in_h = _input.shape[2]
        in_w = _input.shape[3]
        kh = self.kernel_shape[0]
        kw = self.kernel_shape[1]
        sy = self.strides[0]
        sx = self.strides[1]
        dy = self.dilations[0]
        dx = self.dilations[1]

        if self.auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if self.pads is not None:
                pad_h = (self.pads[0], self.pads[2])
                pad_w = (self.pads[1], self.pads[3])
        else:
            pad_h = auto_pad_to_manual_pad(in_h, kh, sy, dy, self.auto_pad)
            pad_w = auto_pad_to_manual_pad(in_w, kw, sx, dx, self.auto_pad)

        out_ch = env[W].shape[0]

        out_h = ((pad_h[0] + in_h + pad_h[1]) - ((kh - 1) * dy + 1)) // sy + 1
        out_w = ((pad_w[0] + in_w + pad_w[1]) - ((kw - 1) * dx + 1)) // sx + 1

        return np.zeros((batch, out_ch, out_h, out_w), dtype=env[x].dtype)

    def to_Edge(self, env, constants):
        b = None
        if len(self.node.input) == 2:
            [x, W] = self.node.input
        elif len(self.node.input) == 3:
            [x, W, b] = self.node.input
        else:
            raise 'invalid'
        if W not in constants:
            raise 'unsupported'
        W = constants[W]
        if b is not None:
            if b not in constants:
                raise 'unsupported'
            b = constants[b]

        _input = env[x]
        batch = _input.shape[0]
        in_ch = _input.shape[1]
        in_h = _input.shape[2]
        in_w = _input.shape[3]
        kh = self.kernel_shape[0]
        kw = self.kernel_shape[1]
        sy = self.strides[0]
        sx = self.strides[1]
        dy = self.dilations[0]
        dx = self.dilations[1]

        if self.auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if self.pads is not None:
                pad_h = (self.pads[0], self.pads[2])
                pad_w = (self.pads[1], self.pads[3])
        else:
            pad_h = auto_pad_to_manual_pad(in_h, kh, sy, dy, self.auto_pad)
            pad_w = auto_pad_to_manual_pad(in_w, kw, sx, dx, self.auto_pad)

        is_depthwise = False
        out_channels,in_channels_per_groups = W.shape[:2]
        if self.group > 1 and 1 == in_channels_per_groups:
            return [
                Edge([x], self.node.output, Function('DepthwiseConvolution2D', {
                    b'W': encode_ndarray(np.rollaxis(W.reshape(self.group,out_channels//self.group,kh,kw),1,0)),
                    b'b': encode_ndarray(b),
                    b'stride': (sy, sx),
                    b'pad_h': pad_h,
                    b'pad_w': pad_w,
                    b'dilate': (dy, dx),
                }))
            ]
        else:
            return [
                Edge([x], self.node.output, Function('Convolution2D', {
                    b'W': encode_ndarray(W),
                    b'b': encode_ndarray(b),
                    b'stride': (sy, sx),
                    b'pad_h': pad_h,
                    b'pad_w': pad_w,
                    b'dilate': (dy, dx),
                    b'groups': self.group,
                }))
            ]

class OpDropout(Op):

    def __init__(self, node):
        super().__init__(node)

    def get_dummy_output(self, env):
        [x] = self.node.input
        return env[x]

    def to_Edge(self, env, constants):
        return [
            Edge(self.node.input, self.node.output[:1], Function('Transpose', {
                b'axes': list(range(len(env[self.node.input[0]].shape)))
            }))
        ]

class OpGemm(Op):

    def __init__(self, node):
        super().__init__(node)

        self.alpha = 1.0
        self.beta = 1.0
        self.transA = 0
        self.transB = 0
        for attr in node.attribute:
            if attr.name == 'alpha':
                self.alpha = attr.f
            if attr.name == 'beta':
                self.beta = attr.f
            if attr.name == 'transA':
                self.transA = attr.i
            if attr.name == 'transB':
                self.transB = attr.i

    def get_dummy_output(self, env):
        [A,B,C] = self.node.input
        a = env[A]
        if self.transA == 1:
            a = a.T
        b = env[B]
        if self.transB == 1:
            b = b.T
        return a.dot(b)+env[C]

    def to_Edge(self, env, constants):
        [A,B,C] = self.node.input

        if B not in constants:
            raise 'unsupported'
        if C not in constants:
            raise 'unsupported'

        b = env[B]
        if self.transB == 0:
            b = b.T
        c = env[C]

        if len(c.shape) == 2 and c.shape[0] != 1:
            raise 'unsupported'

        if self.transA == 1:
            internal_node = "{}_{}".format(A, id(A))
            env[internal_node] = env[A].T
            return [
                Edge([A], [internal_node], Function('Transpose', {
                    b'axes': [1,0]
                })),
                Edge([internal_node], self.node.output, Function('Linear', {
                    b'W': encode_ndarray(self.alpha * b),
                    b'b': encode_ndarray(self.beta * c.ravel())
                }))
            ]
        else:
            return [
                Edge([A], self.node.output, Function('Linear', {
                    b'W': encode_ndarray(self.alpha * b),
                    b'b': encode_ndarray(self.beta * c.ravel())
                }))
            ]

class OpLRN(Op):

    def __init__(self, node):
        super().__init__(node)

        self.alpha = 0.0001
        self.beta = 0.75
        self.bias = 1.0
        self.size = None
        for attr in node.attribute:
            if attr.name == 'alpha':
                self.alpha = attr.f
            if attr.name == 'beta':
                self.beta = attr.f
            if attr.name == 'bias':
                self.bias = attr.f
            if attr.name == 'size':
                self.size = attr.i

    def get_dummy_output(self, env):
        [x] = self.node.input
        return env[x]

    def to_Edge(self, env, constants):
        return [
            Edge(self.node.input, self.node.output, Function('LocalResponseNormalization', {
                b'n': self.size,
                b'k': self.bias,
                b'alpha': self.alpha,
                b'beta': self.beta
            }))
        ]

class OpMatMul(Op):

    def __init__(self, node):
        super().__init__(node)

    def get_dummy_output(self, env):
        [x,W] = self.node.input
        return env[x].dot(env[W])

    def to_Edge(self, env, constants):
        [x,W] = self.node.input
        if W in constants:
            return [
                Edge([x], self.node.output, Function('Linear', {
                    b'W': encode_ndarray(env[W]),
                    b'b': None
                }))
            ]
        else:
            raise 'unsupported'

class OpMaxPool(Op):

    def __init__(self, node):
        super().__init__(node)

        self.kernel_shape = None
        self.auto_pad = b'NOTSET'
        self.pads = None
        self.storage_order = 0
        self.strides = (1,1)
        for attr in node.attribute:
            if attr.name == 'kernel_shape':
                self.kernel_shape = attr.ints
            if attr.name == 'storage_order':
                self.storage_order = attr.i
            if attr.name == 'strides':
                self.strides = attr.ints
            if attr.name == 'auto_pad':
                self.auto_pad = attr.s
            if attr.name == 'pads':
                self.pads = attr.ints

    def get_dummy_output(self, env):
        [x] = self.node.input

        _input = env[x]
        batch = _input.shape[0]
        channel = _input.shape[1]
        in_h = _input.shape[2]
        in_w = _input.shape[3]
        kh = self.kernel_shape[0]
        kw = self.kernel_shape[1]
        sy = self.strides[0]
        sx = self.strides[1]

        if self.auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if self.pads is not None:
                pad_h = (self.pads[0], self.pads[2])
                pad_w = (self.pads[1], self.pads[3])
        else:
            pad_h = auto_pad_to_manual_pad(in_h, kh, sy, 1, self.auto_pad)
            pad_w = auto_pad_to_manual_pad(in_w, kw, sx, 1, self.auto_pad)

        out_h = ((pad_h[0] + in_h + pad_h[1]) - ((kh - 1) * 1 + 1)) // sy + 1
        out_w = ((pad_w[0] + in_w + pad_w[1]) - ((kw - 1) * 1 + 1)) // sx + 1

        return np.zeros((batch, channel, out_h, out_w), dtype=env[x].dtype)

    def to_Edge(self, env, constants):
        [x] = self.node.input

        _input = env[x]
        batch = _input.shape[0]
        channel = _input.shape[1]
        in_h = _input.shape[2]
        in_w = _input.shape[3]
        kh = self.kernel_shape[0]
        kw = self.kernel_shape[1]
        sy = self.strides[0]
        sx = self.strides[1]

        if self.auto_pad == b'NOTSET':
            pad_h = (0, 0)
            pad_w = (0, 0)
            if self.pads is not None:
                pad_h = (self.pads[0], self.pads[2])
                pad_w = (self.pads[1], self.pads[3])
        else:
            pad_h = auto_pad_to_manual_pad(in_h, kh, sy, 1, self.auto_pad)
            pad_w = auto_pad_to_manual_pad(in_w, kw, sx, 1, self.auto_pad)

        return [
            Edge(self.node.input, self.node.output, Function('MaxPooling2D', {
                b'kernel': (kh, kw),
                b'stride': (sy, sx),
                b'pad_h': pad_h,
                b'pad_w': pad_w,
            }))
        ]

class OpPad(Op):

    def __init__(self, node):
        super().__init__(node)

        self.mode = b'constant'
        self.pads = None
        self.value = 0.0
        for attr in node.attribute:
            if attr.name == 'mode':
                self.mode = attr.s
            if attr.name == 'pads':
                self.pads = attr.ints
            if attr.name == 'value':
                self.value = attr.f

        if self.mode != b'constant':
            raise 'unsupported'

    def get_dummy_output(self, env):
        [x] = self.node.input
        n = len(self.pads) // 2
        return np.pad(env[x], list(zip(self.pads[:n], self.pads[n:])), mode='constant', constant_values=self.value)

    def to_Edge(self, env, constants):
        n = len(self.pads) // 2
        return [
            Edge(self.node.input, self.node.output, Function('ConstantPadding', {
                b'pads': list(zip(self.pads[:n], self.pads[n:])),
                b'value': self.value,
            }))
        ]

class OpRelu(Op):

    def __init__(self, node):
        super().__init__(node)

    def get_dummy_output(self, env):
        [x] = self.node.input
        return env[x]

    def to_Edge(self, env, constants):
        return [
            Edge(self.node.input, self.node.output, Function('ReLU'))
        ]

class OpReshape(Op):

    def __init__(self, node):
        super().__init__(node)

    def get_dummy_output(self, env):
        [x,shape] = self.node.input
        return env[x].reshape(list(env[shape]))

    def to_Edge(self, env, constants):
        [x,_] = self.node.input
        [y] = self.node.output
        return [
            Edge([x], self.node.output, Function('Reshape', {
                b'shape': list(map(int, env[y].shape))
            }))
        ]

class OpSoftmax(Op):

    def __init__(self, node):
        super().__init__(node)

        self.axis = 1
        for attr in self.node.attribute:
            if attr.name == 'axis':
                self.axis = attr.i

    def get_dummy_output(self, env):
        [x] = self.node.input
        return env[x]

    def to_Edge(self, env, constants):
        return [
            Edge(self.node.input, self.node.output, Function('Softmax', {b'axis': self.axis}))
        ]

class OpSum(Op):

    def __init__(self, node):
        super().__init__(node)

    def get_dummy_output(self, env):
        xs = map(lambda x: env[x], self.node.input)
        return functools.reduce(lambda a,b: a+b, xs)

    def to_Edge(self, env, constants):
        if len(self.node.input) != 2:
            raise 'unsupported'
        return [
            Edge(self.node.input, self.node.output, Function('Add'))
        ]

class OpTranspose(Op):

    def __init__(self, node):
        super().__init__(node)

        self.perm = None
        for attr in node.attribute:
            if attr.name == 'perm':
                self.perm = attr.ints

    def get_dummy_output(self, env):
        [x] = self.node.input
        return np.transpose(env[x], self.perm)

    def to_Edge(self, env, constants):
        return [
            Edge(self.node.input, self.node.output, Function('Transpose', {
                b'axes': self.perm
            }))
        ]

def evaluate(node, env):
    op_name = 'Op{}'.format(node.op_type)
    if hasattr(sys.modules['__main__'], op_name):
        return getattr(sys.modules['__main__'], op_name)(node).get_dummy_output(env)
    else:
        raise 'unsupported'

class ONNX:
    def __init__(self, path):
        self.model = onnx.load(path)
        onnx.checker.check_model(self.model)
        self.sess = onnxruntime.InferenceSession(path)
        self.nodes = self._reconstruct_value_info()
        self.constant_nodes = self._eval_nodes(self._list_constant_nodes())

    def _reconstruct_value_info(self):
        outputs = list(map(lambda x: x.name, self.sess.get_outputs()))
        def dfs(visited, nodes, result):
            for n in nodes:
                _input = self._find_input(n)
                initializer = self._find_initializer(n)
                if initializer is not None:
                    result[n] = tensor_to_narray(initializer)
                elif _input is not None:
                    result[n] = to_dummy_input(_input)
                else:
                    generator = self._find_generator(n)
                    next_nodes = []
                    if hasattr(generator, 'input'):
                        next_nodes = [ i for i in generator.input if i not in visited ]
                    dfs(visited, next_nodes, result)
                    result[n] = evaluate(generator, result)
                visited.append(n)
        result = {}
        dfs([], outputs, result)
        return result

    def _find(self, p, xs, default=None):
        return next(filter(p, xs), default)

    def _find_initializer(self, name):
        return self._find(lambda n: name == n.name, self.model.graph.initializer)

    def _has_initializer(self, name):
        return self._find_initializer(name) is not None

    def _find_generator(self, name):
        return self._find(lambda n: name in n.output, self.model.graph.node)

    def _find_input(self, name):
        return self._find(lambda n: name == n.name, self.model.graph.input)

    def _has_input(self, name):
        return self._find_input(name) is not None

    def to_MLIR(self):
        inputs = list(map(lambda x: x.name, self.sess.get_inputs()))
        outputs = list(map(lambda x: x.name, self.sess.get_outputs()))
        edges = self._to_MLIR_edges()
        nodes = [ Node(n, self.nodes[n]) for n in set(chain.from_iterable(map(lambda x: x.inputs + x.outputs, edges))) ]

        # rename to C ident (some frameworks don't satisfy the onnx spec.)
        renaming_table = dict([ (n.name, 'v{}'.format(i)) for i,n in enumerate(nodes) ])
        def rename(x):
            return renaming_table[x]
        inputs = list(map(rename, inputs))
        outputs = list(map(rename, outputs))
        def rename_edge(e):
            e.inputs = list(map(rename, e.inputs))
            e.outputs = list(map(rename, e.outputs))
            return e
        edges = list(map(rename_edge, edges))
        def rename_node(n):
            n.name = rename(n.name)
            return n
        nodes = list(map(rename_node, nodes))

        return MLIR(
            model = Model(
                self.model.graph.name,
                {b'name': self.model.producer_name, b'version': self.model.producer_version},
                inputs,
                outputs,
                nodes,
                edges
            ),
        )

    def _eval_nodes(self, nodes):
        m = copy.deepcopy(self.model)
        for n in m.graph.output:
            m.graph.output.remove(n)
        m.graph.output.extend(map(lambda n: narray_to_value_info(n, self.nodes[n]), nodes))
        onnx.save(m, '/tmp/tmp.onnx')
        dummy_sess = onnxruntime.InferenceSession('/tmp/tmp.onnx')
        inputs = dict([ (x.name, self.nodes[x.name]) for x in dummy_sess.get_inputs() ])
        output_names = list(map(lambda x: x.name, dummy_sess.get_outputs()))
        if output_names != []:
            result = dummy_sess.run(output_names, inputs)
        else:
            result = []
        return dict(zip(output_names, result))

    def _to_MLIR_edges(self):
        outputs = list(map(lambda x: x.name, self.sess.get_outputs()))
        visited = []
        edges = []
        while outputs != []:
            o = outputs.pop(0)
            if o in visited:
                continue
            visited.append(o)
            generator = self._find_generator(o)
            if generator is not None:
                edge = self._from_operator_to_edge(generator)
                inputs = list(chain.from_iterable(map(lambda x: x.inputs, edge)))
                outputs += inputs
                edges += edge
            initializer = self._find_initializer(o)
            if initializer is not None:
                print('_from_initializer_to_edge')
                edge = self._from_initializer_to_edge(initializer)
                edges += edge
        return edges

    def _list_constant_nodes(self):
        outputs = list(map(lambda x: x.name, self.sess.get_outputs()))
        def dfs(visited, nodes, result):
            for n in nodes:
                if self._has_initializer(n):
                    result.append(n)
                elif self._has_input(n):
                    pass
                else:
                    generator = self._find_generator(n)
                    next_nodes = []
                    if hasattr(generator, 'input'):
                        next_nodes = [ i for i in generator.input if i not in visited ]
                    dfs(visited, next_nodes, result)
                    if hasattr(generator, 'input'):
                        if all([ i in result for i in generator.input ]):
                            for o in generator.output:
                                result.append(o)
                    else:
                        for o in generator.output:
                            result.append(o)
                visited.append(n)
        result = []
        dfs([], outputs, result)
        return result

    def _from_initializer_to_edge(self, node):
        return [
            Edge([], [node.name], Function('Constant', {b'value': encode_ndarray(self.nodes[node.name])}))
        ]

    def _from_operator_to_edge(self, node):
        op_name = 'Op{}'.format(node.op_type)
        if hasattr(sys.modules['__main__'], op_name):
            return getattr(sys.modules['__main__'], op_name)(node).to_Edge(self.nodes, self.constant_nodes)
        else:
            raise 'unsupported'

def to_dummy_input(x):
    if hasattr(x.type, 'tensor_type'):
        if x.type.tensor_type.elem_type == onnx.TensorProto.FLOAT:
            return np.zeros(tuple(map(lambda d: d.dim_value, x.type.tensor_type.shape.dim)), dtype=np.float32)
        else:
            'unsupported'
    else:
        raise 'unsupported'

def main(args):
    mlir = ONNX(args.input).to_MLIR()
    if False:
        import pprint
        pp = pprint.PrettyPrinter(indent=4)
        pp.pprint(mlir.to_dict())
    mlir.save(args.output)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='ONNX to MLIR Converter')
    parser.add_argument('-o', '--output', dest='output', type=str, required=True,
                        metavar='MLIR', help='MLIR file path')
    parser.add_argument(dest='input', type=str,
                        metavar='ONNX', help='ONNX file path')
    main(parser.parse_args())
